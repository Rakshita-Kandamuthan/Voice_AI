
# Voice AI

In this project, we're diving into the world of Natural Language Processing (NLP) by crafting a top-notch model for transcribing Marathi speech into text. The goal is to build a super accurate system that can understand and convert spoken Marathi into written text. 

## Metric Selection
Our Model Choice: OpenAI's Whisper-large-v3 

When gauging how well a speech recognition system performs, we rely on metrics like Word Error Rate (WER) and Character Error Rate (CER). These metrics are like measuring sticks, helping us understand how accurately the system transcribes spoken words into text. 

#### 1. Why do we prefer using WER and CER? 
Well, they offer a few key advantages: 

- They Speak All Languages: Regardless of the language spoken, these metrics provide a fair assessment. They don't favour one language over another, making comparisons more straightforward. 
- They Tell the Whole Story: Imagine looking at the complete transcript generated by the system. WER and CER consider this whole story, pointing out where errors pop up - whether it's substituting one word for another, skipping something, or adding extra bits. 
- Easy to Understand: Think of these metrics as scores. The lower they are, the better the system is at getting things right. They give us a clear number to compare different systems or versions. 
- Relating to the Real World: In the real world, accuracy matters. WER and CER directly measure how close the system's output is to the correct transcript, which is pretty much what we want in everyday use. 
- Finding the Problems: Not only do these metrics tell us how many errors there are, but they also help us figure out where these mistakes happen. Understanding the types of errors helps us make the system better. 

In a nutshell, these metrics - WER and CER - are popular because they're simple, cover everything, and don't play favourites with languages. They're like the trusty yardsticks for measuring how well a speech recognition system is doing.

#### 2. Why Choose OpenAI's Whisper-large-v3 for Marathi Transcription?
OpenAI's Whisper-large-v3 model can be a promising choice for Marathi transcription due to several reasons:

- Proven Performance Beyond Boundaries: The Whisper-large-v3 model might not have aced Marathi in particular, but it shines brightly in handling diverse speech tasks. Even without direct Marathi training, it boasts competitive scores in Word Error Rates (WER) and Character Error Rates (CER) in a variety of languages, showing its potential for Marathi's intricacies.
- A Multilingual Maverick: Though not tailor-made for Marathi, Whisper-large-v3 flaunts a versatile skill set. Its architecture, while not explicitly designed for Marathi nuances, tackles complex speech patterns like a pro. It's more like an all-rounder, excelling in multilingual contexts, which bodes well for Marathi, too.
- Practicality and Versatility: In the sea of models, Whisper-large-v3 stands out for its balance. It might not be the Marathi specialist, but its adaptability, combined with its not-too-shabby performance, ease of use, and resource-friendly nature, makes it a practical choice. Compared to other options, it seems like the most sensible pick for diving into Marathi transcription.

##  Model Development: 
Crafting a Wizard for Transcribing Speech 

When it comes to the model development, I opted to use the “***AutoModelForSpeechSeq2Seq”*** from the “***transformers”*** library, targeting the **"**openai/whisper-large-v3**"** model. This model is a fantastic choice, especially for speech-to-text tasks. Leveraging this existing architecture means tapping into a powerful framework that's specifically designed for transcribing speech into text. 

This choice was made based on the model's capabilities and suitability for the task at hand. The "openai/whisper-large-v3" model is well-equipped to handle speech sequences and effectively convert them into accurate text representations. It’s a state-of-the-art model, designed with the intricacies of natural language processing in mind. 

By using this model, I aimed to capitalize on its strengths in understanding and transcribing speech, aligning with the goal of creating an efficient and accurate transcription system. The decision to use this particular model was driven by its proven track record and its alignment with the task's requirements.
## Code Implementation: 
Bringing the Transcription Model to Life

The implementation of the transcription model prioritizes clarity, organization, and efficiency. Here's how the code was structured:
### 1. Function load\_model\_and\_processor():
- This function loads a pre-trained model and processor for turning speech into text. It takes in the model's ID, allows an optional Torch data type, and specifies the device for model usage. 
- Using the *AutoModelForSpeechSeq2Seq* class, it loads the model ensuring safe tensor operations. Additionally, it loads the necessary processor with the *AutoProcessor* class linked to the provided model ID. 
- The function returns two objects: the loaded pre-trained model ‘*model’* and the initialized processor ‘*processor’*, enabling immediate use for speech-to-text conversion in subsequent steps or functions.
### 2. Function get\_transcription():
- This function employs the Hugging Face Transformers library, utilizing the AutoModelForSpeechSeq2Seq class for loading a pre-trained speech-to-text conversion model. It operates alongside the AutoProcessor class, handling tokenization and feature extraction necessary for transcription. 
- Using the "automatic-speech-recognition" pipeline from the Transformers library, this function transcribes audio files. It configures the pipeline with the pre-trained model, tokenizer, feature extractor, and relevant parameters such as batch size and token limits. 
- Upon providing the audio file path, it uses the specified pipeline to process the audio, extracting and returning the transcribed text.

### 3. Function load\_reference\_transcriptions():

- This function facilitates the loading and processing of reference transcriptions from the 'trans.txt' file. It reads the content of the 'trans.txt' file located in the specified directory 'audio\_directory' and organizes the data for further use. 
- By iterating through the lines of the file, it splits each line by tab separation ('\t') to extract the audio file name and its corresponding transcription. The function generates a dictionary 'transcriptions' containing audio file names as keys and their respective transcriptions as values, forming a structured reference transcription dataset.
- Finally, it returns the compiled dictionary of reference transcriptions for subsequent comparisons or evaluations.

### 4. Function evaluate\_transcriptions() 

- This function oversees the evaluation process by comparing transcribed text against reference text for multiple audio files. It starts by organizing file names and reference transcriptions, ensuring all necessary data is gathered for evaluation. 
- During the evaluation loop, it computes Word Error Rate (WER) and Character Error Rate (CER) for each transcription against its reference. I've utilized clear print statements to display relevant details, enhancing readability.
### 5. Code Structure and Clarity
- The code follows a logical structure, facilitating a seamless understanding of the transcription and evaluation procedures. 
- Each function has a distinct role, contributing to the overall organization. With strategically placed comments and print statements, the code is designed for clarity and ease of comprehension. 
- It's optimized for efficiency, handling multiple audio files without compromising performance.

This well-commented and organized code ensures that anyone exploring or maintaining this transcription model can effortlessly comprehend and build upon the work presented.
## Execution Environment and Dataset Access Requirements:
Setting Up Your Voice AI Environment

To ensure optimal script performance and seamless dataset access, we recommend utilizing Google Colab's runtime configurations and mounting Google Drive

- For smooth script execution, use Google Colab. It offers different configurations to optimize your script's performance. 
- As the script interacts with datasets in Google Drive, 'mount' Google Drive in Colab. This allows the script to effortlessly read and handle specified datasets stored in your Drive. 
- By using Google Colab and mounting Drive, you create an efficient environment for your script to work flawlessly with your datasets.
## Replicating Our Marathi Speech-to-Text Model: 
A Step-by-Step Guide 

Understanding the importance of transparency and reproducibility in our model's evaluation and in order to ensure that anyone can replicate our results seamlessly, I've crafted comprehensive guidelines. 

Here's how you can replicate our Marathi speech-to-text model's performance using the provided test dataset and code: 

1. **Setup and Prerequisites:** Begin by setting up your environment with the required tools and dependencies. Ensure you have access to Python, the Transformers library, Jiwer, and the necessary hardware resources for model execution. 
1. **Data Preparation**: Organize the provided test dataset containing Marathi speech audio files and their corresponding reference transcriptions. Ensure the audio files are in the ".wav" format for accurate evaluation. 
1. **Code Implementation**: Utilize our provided codebase, ensuring that you've structured your directories similarly to ours. You'll find two primary functions: get\_transcription() and evaluate\_transcriptions(). These functions are pivotal in transcribing Marathi speech and evaluating the model's performance against the reference transcriptions. 
1. **Running the Code**: Execute the code on your machine, following the instructions and comments within the code for a clear understanding of each step. This will initiate the transcription process and evaluate the model's performance against the test dataset. 
1. **Result Analysis and Comparison**: Once the code execution is complete, review the generated results. The output will include metrics such as Word Error Rate (WER) and Character Error Rate (CER), providing insight into the model's accuracy in transcribing Marathi speech.
## Theory
Unveiling insights through the art of transcription analysis.

When the Word Error Rate (WER) is higher than the Character Error Rate (CER), it often means that the errors are happening more when the model tries to understand whole words instead of individual letters. It could be because: 

- Understanding Sentences: Sometimes, the model might struggle with understanding how words fit together in a sentence. This can lead to mistakes even if it gets most of the letters right. 
- Dealing with Complexity: Longer phrases or complex sentences might confuse the model. Even if it's good with single letters, putting them together to form the right words could be a challenge. 
- Accents and Noise: Background noise or different accents can make it harder for the model to catch the right words, even if it hears individual sounds accurately. 
- Grammar Hiccups: Errors might pop up due to issues with grammar or how sentences are structured, causing the model to pick the wrong words. 
- Learning Limits: If the model didn't get to study a wide variety of speech patterns or accents, it might find it tough to understand and write down the right words. 

So, basically, a higher WER suggests that the model might struggle more with understanding how words come together in sentences, even if it's doing a decent job with individual letters.

## Credits 
Empowering with State-of-the-Art NLP Tools.
### Libraries and Tools: 
- Hugging Face Transformers: Utilized for the AutoModelForSpeechSeq2Seq and AutoProcessor to enable speech-to-text transcription. 
- JiWER (jiwer): Employed for calculating Word Error Rate (WER) and Character Error Rate (CER) between transcriptions. 
### External Data: 
- Reference Transcriptions ('trans.txt'): Used as a basis for evaluating transcription accuracy. 
### Python Libraries: 
- os: Used for file path and directory operations. 
- torch: Utilized for device management and data type handling.
### Contributor : Rakshita Kandamuthan
- Single-handedly led the development, implementation, and assessment of the speech-to-text model.
